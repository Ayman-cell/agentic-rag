# ğŸ§  Agentic RAG â€“ Recherche Documentaire + Web avec CrewAI

**Un systÃ¨me Agentic RAG qui combine recherche dans vos documents (PDF) + fallback web, orchestrÃ© par des agents CrewAI, avec support LLM local (Ollama) ou API.**

<div align="center">

## ğŸŒ **[VOIR LA DÃ‰MO (YouTube)](https://youtu.be/O4yBW_GTRk0)** ğŸŒ

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=for-the-badge&logo=python)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red?style=for-the-badge&logo=streamlit)](https://streamlit.io/)
[![CrewAI](https://img.shields.io/badge/CrewAI-Agents-black?style=for-the-badge)](https://docs.crewai.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-VectorDB-ff4d4d?style=for-the-badge&logo=qdrant)](https://qdrant.tech/)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-000000?style=for-the-badge)](https://ollama.com/)

</div>

---

## ğŸ“‹ Vue dâ€™ensemble

Ce projet est une **implÃ©mentation Agentic RAG** (Retrieval-Augmented Generation) construite autour de **CrewAI** :  
au lieu dâ€™un pipeline statique Â« retrieve â†’ generate Â», on orchestre un **workflow multi-agents** capable de :

- ğŸ” **Chercher dans un PDF** (indexation locale + recherche sÃ©mantique)
- ğŸŒ **Basculer vers une recherche web** si la rÃ©ponse nâ€™est pas trouvÃ©e dans les docs
- ğŸ§  **SynthÃ©tiser une rÃ©ponse** propre et structurÃ©e

ğŸ‘‰ Le projet inclut plusieurs variantes dâ€™exÃ©cution :
- **Streamlit (Serper)** : recherche web via `SerperDevTool`
- **Streamlit (Local DeepSeek-R1)** : LLM local via `ollama/deepseek-r1` + web via FireCrawl
- **Streamlit (Local Llama 3.2)** : LLM local via `ollama/llama3.2` + web via FireCrawl

---

## âœ¨ FonctionnalitÃ©s principales

### 1ï¸âƒ£ Agentic RAG (CrewAI)
- ğŸ¤– Orchestration **multi-agents** (process sÃ©quentiel)
- ğŸ§© DÃ©composition des tÃ¢ches : **retrieval â†’ synthesis**
- ğŸ” Workflow extensible (ajout dâ€™outils, routage, mÃ©moire, etc.)

### 2ï¸âƒ£ Recherche dans vos documents (PDF)
- ğŸ“„ Extraction texte PDF via **MarkItDown**
- ğŸ§  Chunking sÃ©mantique via **chonkie (SemanticChunker)**
- ğŸ“¦ Stockage vectoriel **Qdrant en mÃ©moire** (rapide pour dÃ©mos)

### 3ï¸âƒ£ Fallback Web Search
- ğŸ” Via **SerperDevTool** (Google-like search)
- ğŸŒ Ou via **FireCrawlWebSearchTool** (selon la variante)

### 4ï¸âƒ£ Support LLM local (Ollama) ou API
- ğŸ§  ModÃ¨les locaux : `ollama/deepseek-r1`, `ollama/llama3.2`
- ğŸ”‘ Option API (selon configuration)

---

## ğŸ—ï¸ Architecture (simplifiÃ©e)

```
Utilisateur (Streamlit)
   â”‚
   â–¼
Agent Retriever (CrewAI)
   â”œâ”€â”€ ğŸ“„ DocumentSearchTool (PDF â†’ chunks â†’ Qdrant â†’ top matches)
   â””â”€â”€ ğŸŒ Web Search (Serper / FireCrawl)
   â”‚
   â–¼
Agent Synthesizer (CrewAI)
   â”‚
   â–¼
RÃ©ponse finale (citations/segments pertinents)
```

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Technologie | Utilisation |
|---|---|
| **CrewAI** | Orchestration dâ€™agents + tÃ¢ches |
| **Streamlit** | Interface web interactive |
| **MarkItDown** | Extraction texte depuis PDF |
| **chonkie (SemanticChunker)** | DÃ©coupage sÃ©mantique des documents |
| **Qdrant (in-memory)** | Vector store pour la recherche |
| **SerperDevTool** | Recherche web |
| **FireCrawl** | Recherche web / crawling (selon variante) |
| **Ollama** | ExÃ©cution de LLMs locaux |
| **Pydantic** | SchÃ©mas dâ€™entrÃ©es outils |

---

## ğŸ“‹ PrÃ©requis

- Python **3.10+** (conforme au `pyproject.toml`)
- (Optionnel) **Ollama** si vous utilisez les variantes locales
- ClÃ©s API selon votre mode :
  - `SERPER_API_KEY` (recherche web via Serper)
  - `FIRECRAWL_API_KEY` (web via FireCrawl)
  - `OPENAI_API_KEY` (si vous utilisez un modÃ¨le API)

---

## ğŸš€ Installation & DÃ©marrage

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/Ayman-cell/agentic-rag.git
cd agentic-rag
```

### 2ï¸âƒ£ CrÃ©er un environnement virtuel
```bash
python -m venv .venv
# Windows
.\.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
```

### 3ï¸âƒ£ Installer les dÃ©pendances
Le dÃ©pÃ´t dÃ©clare `crewai[tools]` dans `pyproject.toml`.

```bash
pip install -e .
pip install streamlit markitdown "chonkie[semantic]" qdrant-client fastembed
```

> Astuce : si vous prÃ©fÃ©rez, vous pouvez aussi installer directement :
> `pip install "crewai[tools]>=0.86.0,<1.0.0"`

---

## ğŸ” Configuration (.env)

CrÃ©ez un fichier `.env` (ou copiez `.env.example`) :

```bash
cp .env.example .env
```

Exemple (selon vos besoins) :

```dotenv
# Optionnel si vous utilisez OpenAI
MODEL=your_model_name
OPENAI_API_KEY=your_openai_api_key

# Recherche web Serper (app.py)
SERPER_API_KEY=your_serper_api_key

# Recherche web FireCrawl (apps locales)
FIRECRAWL_API_KEY=your_firecrawl_api_key
```

---

## â–¶ï¸ Lancer lâ€™application (Streamlit)

### Variante 1 â€” Serper (web search via SerperDevTool)
```bash
streamlit run app.py
```

### Variante 2 â€” Local DeepSeek-R1 (Ollama) + FireCrawl
```bash
streamlit run app_deep_seek.py
```

### Variante 3 â€” Local Llama 3.2 (Ollama) + FireCrawl
```bash
streamlit run app_llama3.2.py
```

ğŸ“Œ Dans lâ€™UI Streamlit :
- uploadez un PDF dans la sidebar
- posez vos questions dans le chat
- le systÃ¨me tente dâ€™abord le document, puis fallback web si nÃ©cessaire

---

## ğŸ§© Structure du projet

```
agentic-rag/
â”œâ”€â”€ assets/                   # Images / ressources
â”œâ”€â”€ knowledge/                # Documents (PDF) Ã  indexer
â”œâ”€â”€ src/agentic_rag/
â”‚   â”œâ”€â”€ crew.py               # Agents + tÃ¢ches CrewAI (config YAML)
â”‚   â”œâ”€â”€ main.py               # Runner CLI (scripts pyproject)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ agents.yaml       # RÃ´les / objectifs agents
â”‚   â”‚   â””â”€â”€ tasks.yaml        # DÃ©finition des tÃ¢ches
â”‚   â””â”€â”€ tools/
â”‚       â””â”€â”€ custom_tool.py    # DocumentSearchTool (PDF â†’ Qdrant)
â”œâ”€â”€ app.py                    # Streamlit (Serper)
â”œâ”€â”€ app_deep_seek.py          # Streamlit (Ollama DeepSeek-R1 + FireCrawl)
â”œâ”€â”€ app_llama3.2.py           # Streamlit (Ollama Llama 3.2 + FireCrawl)
â”œâ”€â”€ agentic_rag.ipynb         # Notebook dÃ©mo
â”œâ”€â”€ demo_llama3.2.ipynb       # Notebook dÃ©mo
â”œâ”€â”€ .env.example              # Variables dâ€™environnement
â””â”€â”€ pyproject.toml            # DÃ©pendances + scripts
```

---

## ğŸ§  DÃ©tails : DocumentSearchTool

Lâ€™outil `DocumentSearchTool` (dans `src/agentic_rag/tools/custom_tool.py`) :

- extrait le texte du PDF via **MarkItDown**
- crÃ©e des chunks via **SemanticChunker** (`chonkie`)
- utilise un embedding model : `minishlab/potion-base-8M`
- stocke les chunks dans **Qdrant en mÃ©moire** (`QdrantClient(":memory:")`)
- renvoie les passages les plus pertinents pour la requÃªte

> âš ï¸ Note : le mode `:memory:` est idÃ©al pour dÃ©mos/POC. Pour la production, utilisez un Qdrant persistant + gestion dâ€™IDs, mÃ©tadonnÃ©es et versions.

---

## ğŸ› Troubleshooting

- **Ollama ne rÃ©pond pas** : vÃ©rifiez que le service tourne (`http://localhost:11434`).
- **Aucune rÃ©ponse trouvÃ©e** : testez un PDF plus textuel / vÃ©rifiez que lâ€™upload et lâ€™indexation ont bien eu lieu.
- **ClÃ© API manquante** : assurez-vous que `.env` est chargÃ© et que les variables sont dÃ©finies.

---

## ğŸ¤ Contribution

PRs et issues bienvenues âœ…  
IdÃ©es dâ€™amÃ©liorations :
- persistance Qdrant (Docker) au lieu de `:memory:`
- citations + scores de retrieval
- Ã©valuation (RAGAS / tests de rÃ©gression)
- ajout reranking (cross-encoder)
- support multi-documents + metadata filtering

---

## ğŸ“ Licence

Voir le fichier `LICENSE` du dÃ©pÃ´t.

---

**DerniÃ¨re mise Ã  jour : 14 fÃ©vrier 2026**  
**Version : 0.1.0**
